# Park 3: Understanding And Utilizing Attention

Attention Mechanism originated in RNNs in the paper "Neural Machine Translation by Jointly Learning to Align and Translate".

## What are recurrent neural networks

### What is different about these neural networks

### Why is sequential data good

### What is attention and how was it integrated into RNN

### What are the shortcomings of RNN

## What is a transformer

### What is self-attention

### What is cross-attention

### Why are transformers better than RNNs

