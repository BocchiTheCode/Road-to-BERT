# Road-to-BERT

Hello! This repository will be used to store Machine Learning materials that I will be using as reference for teaching others and whatnot.

These materials won't go into details and will rely heavily on abstractions.

## To get started

* Read brief-history-of-machine-learning.md

## General Questions I'm trying to answer

- [ ] Can machines learn to do things on their own
- [ ] How much human intervention is required to enable learning in machines
- [ ] Do machines understand their environment/domain the same way we do
- [ ] What are black boxes and how much light can we throw on them
- [ ] Is deep learning just glorified curve fitting
- [ ] Why are machines able to outperform humans in some tasks
- [ ] If machines can only do numerical computations then how are they handling complex modalities like language
- [ ] Are latent features meaningless
- [ ] If machines are very fast at doing computations then why do we need optimisation algorithms

## Specific things I will cover

* How do we convert text into meaningful numbers that machines can handle
* What all can we do with these embeddings
* What is k nearest neighbor
* What is a perceptron and why do we keep stacking them
* What are activation functions
* What is softmax
* What is backpropagation
* What is gradient descent
* What is a recurrent neural network
* What is attention
* What is a transformer and why do we care about attention so much
* What is BERT, how was it trained, and what can we use it for
