# Part 1: How to handle text input

## Converting text into numbers

All Machine Learning Models are mathematical functions and they can only take numerical input. For a model to be able to handle text data it is necessary that we pass the text through a tokenizer first and then vectorize each token to generate token embeddings.

### What is a tokenizer

Tokenizer is a function that will split the input text into different segments and each segment will be unique.

### Are there different ways of splitting the text

There are different ways in which the text can be split. We can split at character-level, word-level, and also subword-level. Subword-level is when we break existing words into smaller constituents (plugin -> plug + ##in).

### Which splitting technique is more meaningful

Splitting at character-level makes the text lose all meaning. Splitting at word-level is the most meaningful but there are a LOT of words and it would lead to a huge vocabulary. Subword splitting is the most optimal because it leads to small vocabulary while still retaining a lot of the meaning.

### Why is it important for tokens to be meaningful and finite

Machine Learning works by looking for patterns. If the tokens are meaningful and finite- it reduces the search space of the model and it becomes easier for them to look for important/broader patterns.

### How do we get the embeddings of each token

With BERT the token embeddings are learnt during pre-training (which will be covered later). But before BERT there were other more intuitive and simpler ways to generate embeddings for tokens.

The key idea behind GloVe is to capture global statistical information about word co-occurrences in a corpus to generate meaningful word embeddings. It did this by-
1. Taking a huge text corpus and generating a co-occurence matrix.
2. Designing an objective function which minimized the difference between the product of embeddings for words (that co-occur frequently) and their corresponding co-occurrence probabilities in the matrix.
3. GloVe employs matrix factorization techniques to factorize the co-occurrence matrix X into two lower-dimensional matrices W and C. Each row of matrix W represents the vector representation (embedding) of a word as the "target" word, and each row of matrix C represents the vector representation of a word as the "context" word.
4. Adjusting the word embeddings to minimize the objective function using stochastic gradient descent.
5. The learned embeddings are extracted from the rows of the matrix W or C after training.

### Can we directly use these embeddings

Token embeddings capture semantic relationships between Tokens in the training corpus. These embeddings can be used to measure similarity and perform various natural language processing tasks.

### What is K-Nearest Neighbors

You can use k-nearest neighbors (KNN) on token embeddings generated by algorithms like Word2Vec, GloVe, or any other method. KNN is a simple and effective algorithm for finding similar items based on their vector representations.

The KNN algorithm stores the entire training dataset in memory. Each data point in the training set has associated labels for classification tasks or target values for regression tasks.

When a new input query is received, the algorithm calculates the distance between the query point and every point in the training dataset. Common distance metrics include Euclidean distance, Manhattan distance, or cosine similarity.
